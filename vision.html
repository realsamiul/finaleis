<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>M0NARQ Vision | Intelligent Quality Control</title>
    <link rel="stylesheet" href="entry-client-6e158fa6.css">
    <link rel="icon" href="preloader-logo.svg">
    <style>
        :root { --bg-navy: #0E1B2B; --teal: #009CA6; --orange: #FF6B1C; --glass: rgba(255, 255, 255, 0.05); }
        body { background-color: var(--bg-navy); color: #F7F9FA; font-family: 'Sohne', sans-serif; }
        .font-mono { font-family: 'JetBrains Mono', monospace; }
        .text-teal { color: var(--teal); }
        .border-glass { border: 1px solid rgba(255,255,255,0.1); }
        .bg-glass { background: var(--glass); }
        
        details[open] summary ~ * { animation: slideDown 0.3s ease-in-out; }
        @keyframes slideDown { from { opacity: 0; transform: translateY(-10px); } to { opacity: 1; transform: translateY(0); } }
    </style>
</head>
<body class="bg-[#0E1B2B] text-white antialiased">
    <div id="app">
        <header class="fixed top-0 left-0 w-full z-50 mix-blend-difference border-b border-white/10 bg-[#0E1B2B]/80 backdrop-blur-md">
            <div class="container mx-auto px-6 h-20 flex items-center justify-between">
                <a href="index.html" class="block w-32"><img src="logo-white.svg" alt="M0NARQ"></a>
                <nav class="hidden md:flex gap-8 text-sm font-medium uppercase tracking-widest text-white/80">
                    <a href="index.html" class="hover:text-[#009CA6]">Home</a>
                    <a href="powerguard.html" class="hover:text-[#009CA6]">PowerGuard</a>
                    <a href="pulse.html" class="hover:text-[#009CA6]">Pulse</a>
                </nav>
                <a href="#deploy" class="px-6 py-2 bg-[#009CA6] text-white rounded-full text-xs uppercase hover:bg-white hover:text-[#009CA6] transition-all font-bold">Deploy Vision</a>
            </div>
        </header>

        <main data-scroll-container>
            
            <section class="min-h-screen flex items-center pt-20" data-scroll-section>
                <div class="container mx-auto px-6">
                    <div class="grid grid-cols-1 lg:grid-cols-2 gap-16 items-center">
                        <div>
                            <span class="text-[#FF6B1C] text-xs font-bold uppercase tracking-[0.2em] mb-6 block">M0NARQ Vision</span>
                            <h1 class="text-6xl md:text-7xl font-bold mb-8 leading-tight">Precision at <br>the Edge.</h1>
                            <p class="text-xl text-white/60 leading-relaxed mb-10">
                                Stop defects before they become claims. Military-grade detection running on the smartphones you already own.
                            </p>
                            <div class="grid grid-cols-3 gap-8 border-t border-white/10 pt-8">
                                <div>
                                    <div class="text-3xl font-bold text-[#009CA6] font-mono">$390M</div>
                                    <div class="text-[10px] uppercase tracking-widest text-white/40 mt-2">Annual Industry Loss</div>
                                </div>
                                <div>
                                    <div class="text-3xl font-bold text-[#009CA6] font-mono">47 Days</div>
                                    <div class="text-[10px] uppercase tracking-widest text-white/40 mt-2">ROI Payback</div>
                                </div>
                                <div>
                                    <div class="text-3xl font-bold text-[#009CA6] font-mono">28ms</div>
                                    <div class="text-[10px] uppercase tracking-widest text-white/40 mt-2">Inference Speed</div>
                                </div>
                            </div>
                        </div>
                        <div>
                            <div class="bg-black border border-white/20 rounded-lg p-8 shadow-2xl font-mono text-xs">
                                <div class="text-[#FF6B1C] border-b border-white/20 pb-2 mb-4 flex justify-between">
                                    <span>● LIVE STREAM</span>
                                    <span>LINE 04: CAM_01</span>
                                </div>
                                <div class="space-y-3 text-[#9DA7B3]">
                                    <div>>> INIT MicroViT-Tiny-Q8... <span class="text-[#009CA6]">OK</span></div>
                                    <div>>> SENSOR: Android Cam (1080p @ 30fps)</div>
                                    <div>>> BACKBONE: int8 quantized</div>
                                    <br>
                                    <div class="p-4 bg-[#0E1B2B] border border-white/10 rounded">
                                        <div class="text-[#FF6B1C] font-bold mb-2">[ALERT] DEFECT DETECTED</div>
                                        <div class="grid grid-cols-2 gap-4">
                                            <div>TYPE: <span class="text-white">Needle Mark</span></div>
                                            <div>CONF: <span class="text-[#009CA6]">89.4%</span></div>
                                            <div>AREA: <span class="text-white">0.45%</span></div>
                                            <div>LOC: <span class="text-white">[340, 120, 380, 140]</span></div>
                                        </div>
                                    </div>
                                    <br>
                                    <div class="inline-block bg-[#FF6B1C] text-black px-2 py-1 font-bold">ACTION: LINE STOP TRIGGERED</div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="py-24 bg-glass" data-scroll-section>
                <div class="container mx-auto px-6">
                    <div class="grid grid-cols-1 lg:grid-cols-2 gap-16">
                        <div>
                            <h3 class="text-3xl font-bold mb-8 text-[#009CA6]">Hardware Stack (< $150)</h3>
                            <p class="text-white/60 mb-8">We replace $12,000 AOI rigs with commodity hardware.</p>
                            <ul class="space-y-6">
                                <li class="border-b border-white/10 pb-4">
                                    <span class="block text-xs uppercase text-[#FF6B1C] mb-1 tracking-widest">Sensor</span>
                                    <span class="text-lg">Android Smartphone (Galaxy S10+) OR Raspberry Pi 5 + USB Cam</span>
                                </li>
                                <li class="border-b border-white/10 pb-4">
                                    <span class="block text-xs uppercase text-[#FF6B1C] mb-1 tracking-widest">Mount</span>
                                    <span class="text-lg">Basic tripod or 3D-printed rig ($20)</span>
                                </li>
                                <li class="border-b border-white/10 pb-4">
                                    <span class="block text-xs uppercase text-[#FF6B1C] mb-1 tracking-widest">Connectivity</span>
                                    <span class="text-lg">Local Wi-Fi or LoRa for dead spots</span>
                                </li>
                            </ul>
                        </div>
                        <div>
                            <h3 class="text-3xl font-bold mb-8 text-[#009CA6]">The "Vision Map" Process</h3>
                            <div class="space-y-8">
                                <div class="flex gap-6">
                                    <div class="font-mono text-4xl text-white/10 font-bold">01</div>
                                    <div>
                                        <h4 class="font-bold text-xl mb-2">Capture</h4>
                                        <p class="text-white/60">Phone streams high-res video of moving fabric. Auto-exposure algorithms handle variable factory lighting.</p>
                                    </div>
                                </div>
                                <div class="flex gap-6">
                                    <div class="font-mono text-4xl text-white/10 font-bold">02</div>
                                    <div>
                                        <h4 class="font-bold text-xl mb-2">Edge Inference</h4>
                                        <p class="text-white/60"><strong>MicroViT</strong> model analyzes texture. <strong>PicoSAM-2</strong> segments the defect shape instantly on-device.</p>
                                    </div>
                                </div>
                                <div class="flex gap-6">
                                    <div class="font-mono text-4xl text-white/10 font-bold">03</div>
                                    <div>
                                        <h4 class="font-bold text-xl mb-2">Action</h4>
                                        <p class="text-white/60">"Stop Line" relay triggered via GPIO instantly. Alert sent to Floor Manager's WhatsApp.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="py-24" data-scroll-section>
                <div class="container mx-auto px-6 max-w-4xl">
                    <h2 class="text-4xl font-bold mb-12 text-center">Technical Deep Dive</h2>
                    
                    <div class="space-y-4">
                        <details class="bg-glass border-glass rounded-lg group">
                            <summary class="flex justify-between items-center p-6 cursor-pointer list-none">
                                <span class="font-bold text-lg">Why MicroViT-Tiny-Q8 instead of YOLO?</span>
                                <span class="text-[#009CA6] text-2xl transition-transform group-open:rotate-45">+</span>
                            </summary>
                            <div class="p-6 pt-0 text-white/60 leading-relaxed">
                                YOLO is optimized for object bounding boxes, but fabric defects are subtle textural anomalies. We use <strong>MicroViT-Tiny-Q8</strong> (9M params, int8 quantized) which uses "Self-Attention" to understand the context of the weave, detecting anomalies like "slubs" or "shading". It runs at 28ms on a Raspberry Pi CPU—3.6x faster than standard transformers.
                            </div>
                        </details>

                        <details class="bg-glass border-glass rounded-lg group">
                            <summary class="flex justify-between items-center p-6 cursor-pointer list-none">
                                <span class="font-bold text-lg">PicoSAM-2 & Promptable Segmentation</span>
                                <span class="text-[#009CA6] text-2xl transition-transform group-open:rotate-45">+</span>
                            </summary>
                            <div class="p-6 pt-0 text-white/60 leading-relaxed">
                                We use <strong>PicoSAM-2</strong> (1.3M params) for segmentation. This allows for "Human-in-the-Loop" training. A floor manager can tap a new type of defect on the tablet, and the model instantly "snaps" a mask to it, learning the new defect type without cloud retraining.
                            </div>
                        </details>

                        <details class="bg-glass border-glass rounded-lg group">
                            <summary class="flex justify-between items-center p-6 cursor-pointer list-none">
                                <span class="font-bold text-lg">Privacy: Federated Learning</span>
                                <span class="text-[#009CA6] text-2xl transition-transform group-open:rotate-45">+</span>
                            </summary>
                            <div class="p-6 pt-0 text-white/60 leading-relaxed">
                                Your proprietary patterns never leave the factory. We use Federated Learning. Your device shares only mathematical gradients (weight updates) with the central network, not images. Your factory learns to detect a stain found in another facility without ever seeing their data.
                            </div>
                        </details>
                    </div>
                </div>
            </section>

            <footer class="py-20 border-t border-white/10 text-center text-white/40 text-xs uppercase tracking-widest font-mono" data-scroll-section>
                &copy; 2025 M0NARQ Technologies Ltd.
            </footer>
        </main>
    </div>
    <script type="module" src="entry-client-9303cc06.js"></script>
</body>
</html>
